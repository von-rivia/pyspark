from pyspark.sql import SparkSession
from pyspark.sql.functions import sum as _sum, col, expr

# Crear sesi√≥n de Spark
spark = SparkSession.builder.appName("SQLtoPySpark").getOrCreate()

# Cargar las tablas de datos en DataFrames
plancta_contab_fusion_df = spark.table("informacional_prd.bu_syp_in.plancta_contab_fusion")
sdo_contab_fusion_df = spark.table("informacional_prd.bu_syp_in.sdo_contab_fusion")

# 001_Crea_Plan_Cuentas_BSC
plan_cuentas_bsc_df = plancta_contab_fusion_df.select(
    "empresa",
    "cuenta",
    "descripcion",
    "cod_reaj",
    "tipo_moneda",
    "cent_resp",
    "num_cta_sbif_mdr",
    "cod_producto"
)
plan_cuentas_bsc_df.createOrReplaceTempView("001_Plan_Cuentas_BSC")

# 002_Crea_Saldos_Contables_BSC
saldos_contables_bsc_df = sdo_contab_fusion_df.filter(
    (col("sucur") == 9999) & 
    (col("fecha").between(20250101, 20250131))
).select(
    "empresa",
    "sucur",
    "moned",
    "codig",
    "fecha",
    "tipo_moneda",
    "sdo_orig",
    "sdo_peso",
    "prom_orig",
    "prom_peso",
    "sdo_mes_orig",
    "sdo_mes_peso",
    "prom_anual_orig",
    "prom_anual_peso"
).orderBy("fecha")
saldos_contables_bsc_df.createOrReplaceTempView("002_Saldos_Contables_BSC")

# 002_Saldos_Contables_BSC tabla
result_df = saldos_contables_bsc_df.alias("sc").join(
    plan_cuentas_bsc_df.alias("pc"),
    col("sc.codig") == col("pc.cuenta")
).groupBy(
    "sc.codig", 
    expr("cast(pc.descripcion as string)").alias("descripcion1"), 
    "pc.num_cta_sbif_mdr", 
    "sc.moned", 
    "pc.cent_resp", 
    "pc.cod_producto"
).pivot("sc.fecha").agg(_sum("sdo_peso").alias("SumaDesdo_peso"))

# Mostrar el resultado
result_df.show()

# Guardar el DataFrame como una tabla en el workspace de Databricks
result_df.write.mode("overwrite").saveAsTable("sandbox.ganafinanciero.result_df")